{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "assignment_09.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eutWChNzAB89",
        "colab_type": "text"
      },
      "source": [
        "Please fill in your name and that of your teammate.\n",
        "\n",
        "You: Albin Aliu\n",
        "\n",
        "Teammate: Christoph Jutzet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ebQXhhwAB8_",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lS2_86UAB9A",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the ninth lab. Time to start with the famed neural networks. Everything should be fine until you hit the backpropagation algorithm. There are dozens of versions and implementations online, none are simple or straightforward, but in the lecture I tried an explanation that keeps the complexity to a minimum. It may be confusing to find which part does what and how to implement it, so I added a few tips that I hope will limit your debugging time. Enjoy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqObkNJVAB9B",
        "colab_type": "text"
      },
      "source": [
        "### How to pass the lab?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv2I4QTpAB9C",
        "colab_type": "text"
      },
      "source": [
        "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
        "\n",
        "**You need at least 14 points (out of 21 available) to pass** (66%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hxFaeb6AB9D",
        "colab_type": "text"
      },
      "source": [
        "# 1. Fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez9iOL02AB9E",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1 **[1pt]** Describe a real (human) neuron. Use the words \"dendrite\", \"axon\", \"synapses\" and \"spike\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_lTU_euAB9F",
        "colab_type": "text"
      },
      "source": [
        "A real neuron consists of **dendrites**, which are like trees around the **nucleus**, followed by an axon which ends at the axon terminal, where it connects to other **dendrites** or muscles through so called synapses, which are a small gap between the to connection points where neurotransmitter molecules get released by the terminal which can dock on the receptors of a muscle or dendrite. This happens through a **spike**, an eletrical impuls, which goes through the neuron to release the neurotransmitters at the end of the axon. This spike is then forwarded to the next neuron and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHwXuRBnAB9H",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2 **[1pt]** Describe the logistic function (in English). Include/utilize the concept of \"saturation\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTTqW0DvAB9H",
        "colab_type": "text"
      },
      "source": [
        "A logistic function is a function which basically squishes the whole numberline $\\mathbb{R}$ into the open interval $(0,1)$. It's a S shaped function, if you'd plot it on $\\mathbb{R}^2$. It's point of infliction is at $x = 0$ returning the value $0.5$. As mentioned, very high numbers are squashed to 1 and very low (i.e. negative) numbers are squashed to 0 (concept of saturation). The function never touches 0 or 1 but it get's very close, s.t. the function converges for $\\lim_{x \\to \\infty} f(x) = 1$ and $\\lim_{x \\to -\\infty} f(x) = 0$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNhyFq0lAB9I",
        "colab_type": "text"
      },
      "source": [
        "#### 1.3 **[1pt]** Explain the relationship between the human brain, neural networks, and perceptrons (in English)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0V9pSTGyAB9J",
        "colab_type": "text"
      },
      "source": [
        "The human brain consists of neural networks. However, from a biological perspective I'd not say that our artificial neural networks have a lot in common with real neural networks. They're modeled after them but in the end they're still very different. \n",
        "\n",
        "Quote from [towardsdatascience.com](https://towardsdatascience.com/the-differences-between-artificial-and-biological-neural-networks-a8b46db828b7)\n",
        "> Birds have inspired flight and horses have inspired locomotives and cars, yet none of today’s transportation vehicles resemble metal skeletons of living-breathing-self replicating animals.\n",
        "\n",
        "We know that the Perceptron is limited to linear separation and the learning algorithm requires linearly separable data. Neural networks overcome these limtations by extending the Perceptron with nonlinear activation functions and the Backpropagation algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zj7foJBcAB9J",
        "colab_type": "text"
      },
      "source": [
        "#### 1.4 **[2pt]** Write the full equation for a network with structure [1, 3, 2]. How many weights does this network have?\n",
        "\n",
        "- This means one input, one hidden layer of three neurons, and two neurons in the output layer.\n",
        "- You can provide either the linear algebra equation (still, define any vector or matrix you use) or the fully-expanded version (as `act` in the slides).\n",
        "- Feel free to ignore the bias connection for now.\n",
        "- To define multiple equations on individual lines, wrap each one in double dollars (see source of this cell): $$ eq_1=0 $$ $$ eq_2=1 $$\n",
        "- To draw nice matrices, wrap the elements in a `pmatrix` environment then use `&` to tabulate to the next element and `\\\\` to go to the next line:\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "A & B \\\\ C & D\n",
        "\\end{pmatrix}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mB41iG1AB9K",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://i.imgur.com/CNlnncG.png\" alt=\"neuron graph\" width=\"600px\" />\n",
        "\n",
        "> How many weights does this network have?\n",
        "$1 * 3 + 3 * 2 = 9$\n",
        "\n",
        "Weight Matrix/Vectors:\n",
        "\n",
        "$$ W_{in} = \n",
        "\\begin{pmatrix}\n",
        "w_1 \\\\ w_2 \\\\ w_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$ W_{hid} = \n",
        "\\begin{pmatrix}\n",
        "w_4 & w_6 & w_8 \\\\\n",
        "w_5 & w_7 & w_9\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Input: $x = \\begin{pmatrix} x_1 \\end{pmatrix}$, i.e. a number.\n",
        "\n",
        "Neuron equations:\n",
        "\n",
        "$$ n_{hid} = \\sigma(W_{in} \\cdot x) = \n",
        "\\begin{pmatrix}\n",
        "\\sigma (w_1 \\cdot x_1) \\\\\n",
        "\\sigma (w_2 \\cdot x_1) \\\\\n",
        "\\sigma (w_3 \\cdot x_1)\n",
        "\\end{pmatrix} =: \n",
        "\\begin{pmatrix}\n",
        "n_1 \\\\\n",
        "n_2 \\\\\n",
        "n_3\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$$ n_{out} = \\sigma(W_{hid} \\cdot n_{hid}) = \n",
        "\\begin{pmatrix}\n",
        "\\sigma (w_4 \\cdot n_1 + w_6 \\cdot n_2 + w_8 \\cdot n_3)  \\\\\n",
        "\\sigma (w_5 \\cdot n_1 + w_7 \\cdot n_2 + w_9 \\cdot n_3)  \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Activation of $n_1$:\n",
        "$$ act_{n_4} = \\sigma (w_4 \\cdot n_1 + w_6 \\cdot n_2 + w_8 \\cdot n_3)\n",
        "$$\n",
        "\n",
        "$$ act_{n_5} = \\sigma (w_5 \\cdot n_1 + w_7 \\cdot n_2 + w_9 \\cdot n_3) \n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJuR2Z88AB9L",
        "colab_type": "text"
      },
      "source": [
        "#### 1.5 **[2pt]** The network parametrization only defines an _upper bound_ for the network's functional complexity: explain what does this mean (in English). Then also answer: would an overly large network work for a simple problem? Would an overly small network work for a complex problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2C_Wd7nAB9M",
        "colab_type": "text"
      },
      "source": [
        "As explained in 1.6, the UAT says that we can approximate any continuous function on a compact subset of $\\mathbb{R}^n$ using a finite number of neurons. However, we're limited by a finite number of neurons. This is the upper bound of our precision. The more neurons we have, the more precise our approximation becomes. Thus, an overly large network would work for a simple problem. However, and overly small network will not give us any useful approximation. It will give us an approximation but it will not be very precise, thus not useful. So, yes and no. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qngdx5usAB9M",
        "colab_type": "text"
      },
      "source": [
        "#### 1.6 **[1pt]** Explain the implications of the Universal Approximation theorem (in English)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbdcjoCdAB9N",
        "colab_type": "text"
      },
      "source": [
        "Simply said: we can approximate any continous function (on a compact subset of $\\mathbb{R}^n$) with arbitrary precision ($\\epsilon$) using a neural network. The bigger the network is, the more precise our approximation becomes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-z6iG7VAB9N",
        "colab_type": "text"
      },
      "source": [
        "# 2. Multilayer feed-forward neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWjtAOK6AB9O",
        "colab_type": "text"
      },
      "source": [
        "As is customary, in this section you get to code neural networks by hand. There are many possible implementation; we will focus on a toy version without parallelism or GPUs, but employing Numpy and linear algebra."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDZD5GxWs6eD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e37003a5-7419-4327-fe4a-3c70fcd52930"
      },
      "source": [
        "#load libs\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import math\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9uLVGBYAB9O",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 **[2pt]** Implement a simple neural network. Print the activation for a network with logistic transfer, structure [4,3,2,3], random weights, and input [1,2,1,2].\n",
        "\n",
        "- You need a Python function that activates the whole network on an input, and returns its output (remember: a vector).\n",
        "- All the complexity is just in making the dimensions right. For example, it is useful at the beginning to `assert` the dimension of the input, to catch problems when calling it later.\n",
        "- You can implement a bias for each neuron, but it is probably simpler to first make it work without.\n",
        "- Remember you want the number of neurons in the rows, the number of inputs to the neurons in the columns; so if the structure is [2,3,1], the first matrix is between a layer of size 2 and a layer of size 3, which has 3 neurons with 2 inputs each, and you should produce a weight matrix of size (3,2). The second weight matrix will have size (1,3).\n",
        "- I typically compute/use the following intermediate information:\n",
        "    - `struct`: defines the list of sizes of layers as described above\n",
        "    - `nlayers`: size of struct\n",
        "    - `nins`, `nhids`, `nouts`: deconstructing struct for easier access\n",
        "    - `state`: list of inputs/outputs, size `nlayers+1`. First element holds the network input, the others the (vectorial) outputs of each layer.\n",
        "    - `wsizes`: I construct the sizes of the weight matrices as pairs `[nrows, ncols]`\n",
        "    - `weights`: list of weight matrices, one per layer (the weights are for the layer's inputs)\n",
        "- For testing, remember to pass numpy arrays as input, not python lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp5nLiqo68vC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# sigmoid function\n",
        "sigmoid = lambda x: 1 / (1 + math.exp(-x))\n",
        "sigmoid_derivative = lambda x: sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# activation function\n",
        "activation = lambda x: sigmoid(x)\n",
        "activation_derivative = lambda x: sigmoid_derivative(x)\n",
        "\n",
        "# vectorized activation\n",
        "v_activation = lambda pot: [ activation(x) for x in pot ]\n",
        "v_activation_derivative = lambda pot: [ activation_derivative(x) for x in pot ]\n",
        "\n",
        "# calculate random weights matrices based on struct\n",
        "def rand_weights(struct):\n",
        "  list_of_matrices = []\n",
        "  for i in range(len(struct) - 1):\n",
        "    list_of_matrices.append( 1 * np.random.random((struct[i+1],struct[i])) - 0.5 ) \n",
        "  \n",
        "  return list_of_matrices\n",
        "\n",
        "#return zero weights matrices based on struct\n",
        "def zero_weights(struct):\n",
        "  list_of_matrices = []\n",
        "  for i in range(len(struct) - 1):\n",
        "    list_of_matrices.append(np.zeros((struct[i+1],struct[i])))\n",
        "  \n",
        "  return list_of_matrices"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKjW-XObAB9O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "df8090a7-334f-45df-ea46-8d00e2939f84"
      },
      "source": [
        "#structure for our neural network\n",
        "struct = [4,3,2,3]\n",
        "weights = rand_weights(struct)\n",
        "\n",
        "\n",
        "def forward_pass(struct, weights, inp):\t\n",
        "\n",
        "  #debug mode\n",
        "  debug = False\n",
        "\n",
        "  #compute intermediate data\n",
        "  nlayers = len(struct) - 1\n",
        "\n",
        "  nins = struct[0]\n",
        "  nhids = struct[1:-1]\n",
        "  nouts = struct[-1]\n",
        "  firing_rate = [0] * (nlayers+1)\n",
        "  membrane_potential = [0] * (nlayers+1)\n",
        "  firing_rate[0] = inp;\n",
        "  membrane_potential[0] = inp;\n",
        "\n",
        "  if(debug):\n",
        "    print(\"Computation of neural network:\\n\\nRandom weights matrices:\")\n",
        "    pprint.pprint(weights)\n",
        "\n",
        "  #forward feeding\n",
        "  for i in range(nlayers): #for each layer\n",
        "    #compute\n",
        "    membrane_potential[i+1] = np.dot(weights[i],firing_rate[i])\n",
        "   \n",
        "    #apply sigmoid function\n",
        "    firing_rate[i+1] = np.array([ activation(x) for x in membrane_potential[i+1] ])\n",
        "\n",
        "    if(debug):\n",
        "      print(\"\\nComputing layer \"+str(i)+\":\")\n",
        "      pprint.pprint(firing_rate[i+1])\n",
        "\n",
        "\n",
        "  if(debug): \n",
        "    print(\"\\nLayers computed.\\n\\n\")\n",
        "\n",
        "  #return data\n",
        "  return (np.array(firing_rate),np.array(membrane_potential))\n",
        "\n",
        "\n",
        "fp = forward_pass(struct, weights, [1,2,1,2])\n",
        "\n",
        "activation_values = fp[0][-1]\n",
        "\n",
        "fp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([list([1, 2, 1, 2]), array([0.52999679, 0.48612714, 0.71952378]),\n",
              "        array([0.53353014, 0.42285958]),\n",
              "        array([0.51293398, 0.49627379, 0.45745754])], dtype=object),\n",
              " array([list([1, 2, 1, 2]), array([ 0.12013142, -0.05550567,  0.94210063]),\n",
              "        array([ 0.13432216, -0.31104544]),\n",
              "        array([ 0.05174745, -0.0149051 , -0.17058228])], dtype=object))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THkGp5AvAB9R",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2 **[5pt]** Implement the online Backpropagation algorithm to work with your network implementation.\n",
        "\n",
        "- Keep it simple, use the implementation seen in the slides, based on logistic function and MSE loss.\n",
        "- You need two update functions: one for the output layer, and one for the hidden ones.\n",
        "- If you follow this implementation, you don't need a vectorized version of the activation function. But if you want it to work with numpy broadcast you should learn about `np.vectorize(act_net, signature='(n)->(m)')`.\n",
        "- Careful with the size of inputs and outputs you generate for testing, remember the structure of the network defined above.\n",
        "- The online implementation should save you from one layer of linear algebra: loop for number of epochs, then loop for each point (pairing input/label with `zip`). Then forward pass, backward on output layer, (loop of) backward on hidden layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnbLmcrTAB9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# backgrpopagation\n",
        "\n",
        "\n",
        "# This will compute the delta for the output layer, thus this will return a vector\n",
        "def delta_out(v, y): #i.e. δ^(l)\n",
        "\n",
        "  #renaming, to fit our course formulas\n",
        "  v_l = v[-1]\n",
        "\n",
        "  #check dimensions\n",
        "  assert len(v_l) == len(y), \"The given output layer has not the same dimension as the given label.\"\n",
        " \n",
        "  ones = np.array([1]*len(y))\n",
        "\n",
        "  ret = (v_l - y) * (ones - v_l) * v_l # * should do the hadamard operation\n",
        "  return ret\n",
        "\n",
        "\n",
        "# Delta for hidden layers\n",
        "def delta_hid(k, prop_delta, weights, u): #i.e. δ^(k)\n",
        "  assert k < len(u), \"Apply the delta_hid function only on hidden layers, not the output layer.\"\n",
        "  \n",
        "  #return np.dot(np.array(weights[k+1]).transpose(), prop_delta) * v_activation_derivative(u[k+1])\n",
        "  return np.dot(np.array(weights[k+1]).transpose(), prop_delta) * v_activation_derivative(u[k+1])\n",
        "\n",
        "\n",
        "def backprop(struct, weights, inp):\n",
        "\n",
        "  #declare vars\n",
        "  x = inp[0]\n",
        "  y = inp[1]\n",
        "  n_hidden_layers = len(struct) - 2 # -2 bc input and output layer\n",
        "\n",
        "  deltas = []\n",
        "  dEdWk = []\n",
        "\n",
        "#1. forward pass\n",
        "  fp = forward_pass(struct, weights, x)\n",
        "  v = fp[0]\n",
        "  u = fp[1]\n",
        "\n",
        "#2. delta and grad for output layer \n",
        "\n",
        "  #compute delta\n",
        "  d_out = delta_out(v, y)\n",
        "  deltas.append(d_out)\n",
        "\n",
        "  #compute grad\n",
        "  part_derv_out = np.outer(deltas[0], v[-2])\n",
        "  dEdWk.append(part_derv_out)\n",
        "\n",
        "#3. delta and gradient for hidden layers\n",
        "  d_last = d_out\n",
        "  for i in range(n_hidden_layers-1,-1,-1):\n",
        "\n",
        "    #compute deltas for hidden layer\n",
        "    d_hid = delta_hid(i, d_last, weights, u)\n",
        "\n",
        "    #save for next layer\n",
        "    d_last = d_hid\n",
        "\n",
        "    #add to to our list of deltas\n",
        "    deltas.append(d_hid)\n",
        "\n",
        "    #compute part deriv\n",
        "    #print(\"\\n\\n delta\"+str(i))\n",
        "    #pprint.pprint(d_hid)\n",
        "    #print(\"v\"+str(i))\n",
        "    #pprint.pprint(v[i])\n",
        "    part_derivative = np.outer(d_hid, v[i]) # here, deltas start at 0 but correspond to v[1], we'er calc hiere d^(i-1) * a^(i)\n",
        "    dEdWk.append(part_derivative)\n",
        "\n",
        "  #finally reverse, since we went backwards\n",
        "  deltas.reverse()\n",
        "  dEdWk.reverse()\n",
        "\n",
        "  #pprint.pprint(weights)\n",
        "  #print.pprint(dEdWk)\n",
        "  return dEdWk\n",
        "\n",
        "\n",
        "inp = ([5.8, 2.8, 5.1, 2.4], [0, 1, 0])\n",
        "\n",
        "#g = backprop(struct, weights, inp)\n",
        "#g\n",
        "\n",
        "##struct = [4,3,2,3]\n",
        "##v = forward_pass(struct, weights, [1,2,1,2])\n",
        "#delta_out(v, [0, 1, 0])\n",
        "\n",
        "#g = backprop(struct, weights, inp)\n",
        "#g"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay_MTen6AB9U",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3 **[3pt]** Train a network to classify the Iris dataset using your implementation.\n",
        "\n",
        "- We have three classes, we need to train 3 neurons on 4 inputs. No need for hidden layers.\n",
        "- To match the labels to the output, encode the (discrete) class using one-hot encoding. I suggest you use `pd.get_dummies()`. (Note: for prediction we would typically use `np.argmax()` on the network output)\n",
        "- Remember to drop the `species` column from the dataframe, and merge the dummy variables with one-hot encoding using `pd.merge()`. You want to align the rows/indices, using `left_index=True` and `right_index=True`.\n",
        "- We are back to SL so you need both the `x` (for the forward pass) and the `y` (for the backprop). Then you are ready for the split.\n",
        "- Your implementation of backprop may have problems with dataframes, in which case convert its inputs using `to_numpy()`.\n",
        "- Writing a NN class simplifies greatly the introduction of the backprop code. However you are going to use the code only twice, so copy+paste is also acceptable. Just remember if not that you will need to define a new `struct` here, and that all the variables depending on it (and methods that use those outside-defined variables) should be redefined too. You are in for some nasty bugs if not, try killing the Jupyter kernel often and running only what you need.\n",
        "- Feel free to experiment with learning rates. You can start with 0.1, but you could go as low as $10^{-5}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNaAeEWhAB9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load our data\n",
        "df = sns.load_dataset('iris')\n",
        "dummies = pd.get_dummies(df['species'])\n",
        "df = pd.merge(df, dummies, right_index=True, left_index=True)\n",
        "df = df.drop(columns=['species'])\n",
        "\n",
        "#do the splitting\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2) # 80-20 split\n",
        "\n",
        "x_train = train.iloc[:,:4].to_numpy()\n",
        "y_train = train.iloc[:,4:].to_numpy()\n",
        "\n",
        "x_test = test.iloc[:,:4].to_numpy()\n",
        "y_test = test.iloc[:,4:].to_numpy()\n",
        "\n",
        "\n",
        "#generate network random weights to start with\n",
        "struct = [4,3]\n",
        "initweights = rand_weights(struct)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uu3HDHb4mui_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# neural network implementation\n",
        "\n",
        "#structure for our neural network\n",
        "weights = initweights\n",
        "\n",
        "#the weights defined above, because it's random. we want to keep track for testing\n",
        "\n",
        "for i in range(0,1):\n",
        "  for x,y in zip(x_train, y_train):\n",
        "    inp = (x, y)\n",
        "    \n",
        "    # backrpopagate\n",
        "    dEdWk = backprop(struct, weights, inp)\n",
        "    eta = 0.2\n",
        "\n",
        "    weights = np.array(weights) - eta * np.array(dEdWk)\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65Nc1LrLNTS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5e3c7f6-f13c-47fa-82e5-762fa216ad88"
      },
      "source": [
        "#let's test it\n",
        "\n",
        "errors = 0\n",
        "right = 0\n",
        "\n",
        "for x,y in zip(x_test, y_test):\n",
        "  pred = forward_pass(struct, weights, x)[0][-1]\n",
        "\n",
        "  mask = [0,0,0]\n",
        "  index = np.where(pred == max(pred))\n",
        "  mask[index[0][0]] = 1\n",
        "\n",
        "  if(np.array_equal(y, mask)):\n",
        "    #print(\"OK\")\n",
        "    right+=1\n",
        "  else:\n",
        "    #print(\"ERROR\")\n",
        "    errors+=1\n",
        "\n",
        "  #pprint.pprint(pred)\n",
        "  #pprint.pprint(y)\n",
        "  #print(\"\\n\\n\")\n",
        "\n",
        "\n",
        "print(\"RIGHT / TOTAL: \"+str(right)+\"/\"+str(len(x_test))+\" => \"+str(100*(right/len(x_test)))+\"%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RIGHT / TOTAL: 22/30 => 73.33333333333333%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UO4xn72AB9X",
        "colab_type": "text"
      },
      "source": [
        "# 3. First taste of Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIfBrIi1AB9X",
        "colab_type": "text"
      },
      "source": [
        "I selected to use Keras here simply because it will be more easily available for everyone using Colab. You should be aware of Pytorch as a solid alternative. The trade off is typically between something easier to use for a quick prototype (e.g. Pytorch) vs. something that scales to bigger and more complex; Keras if founded on Tensorflow, which means more complexity but more powerful tools (and Google support), though for a course (and for quick sketches) I would have rather recommended Pytorch. Feel free to use either -- be flexible with your tools!\n",
        "\n",
        "**IF YOU USE PIPENV:** you need to install the right package. I choose not to distribute an updated version of the Pipfile to provide a chance for you to use the `pipenv install` command, and to allow to install whichever library works for your particular system. Since the solutions will be with Keras, doing this homework with Pytorch enables you to see both versions. You will see they don't differ much for toy problems like this anyway."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r-n3TJoAB9X",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 **[2pt]** Train a network to classify the Iris dataset using Keras, and print the trained model accuracy.\n",
        "\n",
        "- Use a sequential model with only one dense layer\n",
        "- The number of neurons in the model depends on the number of classes\n",
        "- The first layer needs the `input_dim` parameter\n",
        "- Use sigmoid activation\n",
        "- After finishing constructing the model, you need to `compile` it using an optimize, a loss and a (list of) metric(s). Use stochastic gradient descent, mean squared error, and accuracy, respectively.\n",
        "- The next step is the training (`fit`). Pass a `validation_split` and it will take care of the split itself, plus it will allow visualizing the performance of the model on the test set at each epoch.\n",
        "- You also want to pass `epochs` and `batch_size`. Values of `1000` and `5` work well, but feel free to experiment.\n",
        "- Finally to print the model accuracy you will need to call `evaluate`, which will pick the `accuracy` measure from when you compiled the model.\n",
        "- Use fewer epochs to test the code faster, 10-100 should work fine.\n",
        "- You may need to restart several time to get lucky with the initialization. Yes this is actually needed, and accepted as common practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UYHverjAB9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9601f909-757b-403d-e014-05e3001e4547"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units = 3, input_dim = 4, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x = df.iloc[:,:4].to_numpy()\n",
        "y = df.iloc[:,4:].to_numpy()\n",
        "\n",
        "model.fit(x, y, validation_split=0.2, epochs=50, batch_size=5)\n",
        "model.evaluate(x,y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 120 samples, validate on 30 samples\n",
            "Epoch 1/50\n",
            "120/120 [==============================] - 0s 703us/step - loss: 0.6430 - accuracy: 0.4167 - val_loss: 0.6663 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - 0s 235us/step - loss: 0.6377 - accuracy: 0.4167 - val_loss: 0.6663 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - 0s 207us/step - loss: 0.6296 - accuracy: 0.4167 - val_loss: 0.6664 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.6162 - accuracy: 0.4167 - val_loss: 0.6667 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "120/120 [==============================] - 0s 233us/step - loss: 0.5917 - accuracy: 0.4167 - val_loss: 0.6681 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "120/120 [==============================] - 0s 257us/step - loss: 0.5458 - accuracy: 0.4167 - val_loss: 0.6782 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "120/120 [==============================] - 0s 223us/step - loss: 0.4852 - accuracy: 0.4167 - val_loss: 0.7164 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "120/120 [==============================] - 0s 232us/step - loss: 0.4489 - accuracy: 0.4167 - val_loss: 0.7626 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "120/120 [==============================] - 0s 238us/step - loss: 0.4370 - accuracy: 0.4167 - val_loss: 0.7929 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "120/120 [==============================] - 0s 207us/step - loss: 0.4331 - accuracy: 0.4167 - val_loss: 0.8103 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "120/120 [==============================] - 0s 210us/step - loss: 0.4315 - accuracy: 0.4167 - val_loss: 0.8195 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "120/120 [==============================] - 0s 237us/step - loss: 0.4307 - accuracy: 0.4167 - val_loss: 0.8260 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "120/120 [==============================] - 0s 237us/step - loss: 0.4301 - accuracy: 0.4167 - val_loss: 0.8305 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "120/120 [==============================] - 0s 216us/step - loss: 0.4296 - accuracy: 0.4167 - val_loss: 0.8324 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.4292 - accuracy: 0.4167 - val_loss: 0.8339 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "120/120 [==============================] - 0s 226us/step - loss: 0.4290 - accuracy: 0.4167 - val_loss: 0.8344 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "120/120 [==============================] - 0s 229us/step - loss: 0.4287 - accuracy: 0.4167 - val_loss: 0.8341 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "120/120 [==============================] - 0s 239us/step - loss: 0.4283 - accuracy: 0.4167 - val_loss: 0.8344 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "120/120 [==============================] - 0s 223us/step - loss: 0.4281 - accuracy: 0.4167 - val_loss: 0.8342 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.4278 - accuracy: 0.4167 - val_loss: 0.8344 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.4275 - accuracy: 0.4167 - val_loss: 0.8330 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "120/120 [==============================] - 0s 210us/step - loss: 0.4273 - accuracy: 0.4167 - val_loss: 0.8322 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.4269 - accuracy: 0.4167 - val_loss: 0.8313 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "120/120 [==============================] - 0s 211us/step - loss: 0.4268 - accuracy: 0.4167 - val_loss: 0.8304 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "120/120 [==============================] - 0s 220us/step - loss: 0.4266 - accuracy: 0.4167 - val_loss: 0.8286 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.4262 - accuracy: 0.4167 - val_loss: 0.8265 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "120/120 [==============================] - 0s 230us/step - loss: 0.4260 - accuracy: 0.4167 - val_loss: 0.8251 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "120/120 [==============================] - 0s 218us/step - loss: 0.4258 - accuracy: 0.4167 - val_loss: 0.8241 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "120/120 [==============================] - 0s 248us/step - loss: 0.4256 - accuracy: 0.4167 - val_loss: 0.8216 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "120/120 [==============================] - 0s 212us/step - loss: 0.4253 - accuracy: 0.4167 - val_loss: 0.8201 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/50\n",
            "120/120 [==============================] - 0s 227us/step - loss: 0.4251 - accuracy: 0.4167 - val_loss: 0.8188 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "120/120 [==============================] - 0s 231us/step - loss: 0.4249 - accuracy: 0.4167 - val_loss: 0.8175 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "120/120 [==============================] - 0s 219us/step - loss: 0.4247 - accuracy: 0.4167 - val_loss: 0.8166 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "120/120 [==============================] - 0s 225us/step - loss: 0.4244 - accuracy: 0.4167 - val_loss: 0.8162 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "120/120 [==============================] - 0s 228us/step - loss: 0.4242 - accuracy: 0.4167 - val_loss: 0.8167 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.4241 - accuracy: 0.4167 - val_loss: 0.8148 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "120/120 [==============================] - 0s 205us/step - loss: 0.4239 - accuracy: 0.4167 - val_loss: 0.8139 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "120/120 [==============================] - 0s 208us/step - loss: 0.4237 - accuracy: 0.4167 - val_loss: 0.8120 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "120/120 [==============================] - 0s 215us/step - loss: 0.4235 - accuracy: 0.4167 - val_loss: 0.8117 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "120/120 [==============================] - 0s 217us/step - loss: 0.4233 - accuracy: 0.4167 - val_loss: 0.8106 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "120/120 [==============================] - 0s 236us/step - loss: 0.4232 - accuracy: 0.4167 - val_loss: 0.8101 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "120/120 [==============================] - 0s 204us/step - loss: 0.4230 - accuracy: 0.4167 - val_loss: 0.8094 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "120/120 [==============================] - 0s 206us/step - loss: 0.4227 - accuracy: 0.4167 - val_loss: 0.8081 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "120/120 [==============================] - 0s 210us/step - loss: 0.4225 - accuracy: 0.4167 - val_loss: 0.8072 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "120/120 [==============================] - 0s 214us/step - loss: 0.4224 - accuracy: 0.4167 - val_loss: 0.8067 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "120/120 [==============================] - 0s 218us/step - loss: 0.4222 - accuracy: 0.4167 - val_loss: 0.8064 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "120/120 [==============================] - 0s 218us/step - loss: 0.4221 - accuracy: 0.4167 - val_loss: 0.8049 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "120/120 [==============================] - 0s 229us/step - loss: 0.4219 - accuracy: 0.4167 - val_loss: 0.8045 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "120/120 [==============================] - 0s 262us/step - loss: 0.4217 - accuracy: 0.4167 - val_loss: 0.8027 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "120/120 [==============================] - 0s 246us/step - loss: 0.4215 - accuracy: 0.4250 - val_loss: 0.8024 - val_accuracy: 0.0000e+00\n",
            "150/150 [==============================] - 0s 54us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4975080116589864, 0.3400000035762787]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2LVL0UKAB9Z",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 **[1pt]** Visualize the model's accuracy and loss over time\n",
        "\n",
        "- You can find a practical example [[here]](https://keras.io/visualization/#training-history-visualization).\n",
        "- You should expect the accuracy to grow, the loss to decrease, and train and test performance to be related but different.\n",
        "- Also with only 3 classes and few data points it's perfectly normal for the accuracy lines to look \"discretized\" (like a step function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jto6Qia2AB9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUzYsRGYAB9b",
        "colab_type": "text"
      },
      "source": [
        "# At the end of the exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAY7Ky9nAB9b",
        "colab_type": "text"
      },
      "source": [
        "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
        "\n",
        "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvkggvA0AB9b",
        "colab_type": "text"
      },
      "source": [
        "#### BONUS **[ZERO pt]** This exercise should already blur the line between \"classification\" and \"regression\". Go all the way and learn to predict the value of one of the four continuous features of the Iris dataset based on the other three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHt7nuxTAB9c",
        "colab_type": "text"
      },
      "source": [
        "#### BONUS **[ZERO pt]** A classic example is the XOR problem: write a neural network that maps two binary inputs to one binary output, and learn the 2D [XOR](https://en.wikipedia.org/wiki/Exclusive_or) logical operation. If you draw the four points, you will see they are not linearly separable. However you can write a neural network with one hidden layer of two neurons that solves the problem. Initialize the network with random weights, then execute the backpropagation algorithm by hand on paper until you solve it. This is a great exercise if you're stuck with the implementation of backprop and you cannot figure out what went wrong, as it forces you to get the dimensions right. Using 3 hidden neurons is a bit simpler and should require less iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTxHVQpVAB9c",
        "colab_type": "text"
      },
      "source": [
        "### Final considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTMQUzTOAB9c",
        "colab_type": "text"
      },
      "source": [
        "- The most important take-home message here is to distinguish between the _model_ and the _learning_. You will find most people use \"neural network\" to refer to both together, which limits the understanding of either part's limitations and applicability."
      ]
    }
  ]
}