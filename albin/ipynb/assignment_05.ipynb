{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"assignment_05.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"4xCye8C34Ug8","colab_type":"text"},"source":["Please fill in your name and that of your teammate.\n","\n","You: Albin Aliu\n","\n","Teammate: Christoph Jutzet"]},{"cell_type":"markdown","metadata":{"id":"pRwT9j-G4Ug-","colab_type":"text"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"MMt1H1d94UhA","colab_type":"text"},"source":["Welcome to the fifth lab. Last week we had a break from math-heavy assignments to allow you to catch up on the first 3 assignments. As far as learning the basics of Pandas could be considered a break anyway. We will learn more of this library over the weeks, hope you will develop an appreciation as you build up experience. \"You may find its methods disagreeable, but you can't avoid appreciating the results\" (cit.)"]},{"cell_type":"markdown","metadata":{"id":"kM4JtJR54UhC","colab_type":"text"},"source":["## Grouping"]},{"cell_type":"markdown","metadata":{"id":"4zGa8cLj4UhD","colab_type":"text"},"source":["You should have some fundamental experience on all libraries used so far at this point (keep the past exercises at hand as reference). It is time to introduce to you the math applications of Pandas DataFrame and Series, and to the unfriendly but oh-so-useful `groupby()`.\n","\n","From now on we will be using Pandas for our main data structures, even in the math calculation. Remember that they wrap around Numpy arrays while giving convenient indexing and extra capabilities. No need to e.g. split the points depending on the class into a `dict` as we did before: we can _group_ data by label, then all operations will work on the whole feature arrays and for all classes at once (and running the underlying, optimized C implementation).\n","Simply treat a DataFrame just like you would a multi-dimensional Numpy array (and a Series as if it was a one-dimensional Numpy array). Function calls will be _broadcasted_ to its elements.\n","\n","We will use `groupby()` extensively. It can be slow to grasp at first, but your code will be legible and you will need (almost) no more loops nor `dict`s. \n","Careful because it returns [a `GroupBy` object](https://pandas.pydata.org/pandas-docs/stable/reference/groupby.html) that is somehow unwieldy: it removes a feature, adds a dimension (the grouping), and does not print directly. Try to follow it with a `describe()` to see its application, or print the `groups` for an intuition on how it works: basically a `dict` from each element of the group (e.g. the classes) to the indices of the corresponding rows. Basically it's just an implicit split on the data, and now any operation you call on the GroupBy object will return multiple results (one per group) instead of just one. Go ahead and give it a try, understanding this is necessary before moving on to the next questions, and it's easier if you play with it yourself: the trick that did it for me was to try and ignore its output *per se*, and instead just call functions on it such as `describe()`. \n","\n","You need to start thinking of the data as a whole, high-dimensional entity, and your exploration as selecting and slicing this object from different perspectives as if you were \"floating around it in space\". When using the DataFrame for math just remember that you are manipulating multiple variables at the same time: treat it like a special Numpy data structure and everything should be intuitive."]},{"cell_type":"markdown","metadata":{"id":"awxqyJ3m4UhE","colab_type":"text"},"source":["### How to pass the lab?"]},{"cell_type":"markdown","metadata":{"id":"MmbPwKO04UhF","colab_type":"text"},"source":["Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n","\n","**You need at least 12 points (out of 18 available) to pass** (66%)."]},{"cell_type":"code","metadata":{"id":"ES1PbQ9s4UhG","colab_type":"code","colab":{}},"source":["# Let me hit ctrl+c ctrl+v for you\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import pandas as pd\n","sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eI_uE61x4UhJ","colab_type":"text"},"source":["# 1. Fundamentals"]},{"cell_type":"markdown","metadata":{"id":"TO3AEUr34UhK","colab_type":"text"},"source":["This time we start strong with an example that is simple but longer. Take your time to read and understand each part, follow the suggestions, and it should unravel without much trouble."]},{"cell_type":"markdown","metadata":{"id":"bGr9wNji4UhL","colab_type":"text"},"source":["#### 1.1 **[4pt]** When the weather forecast says it is going to rain, 15% of the time they are wrong. When they say it is not going to rain, they are wrong 5% of the times. Also, hypothesize that in the current season you get rain on 20% of the days. Using Bayes' rule, calculate by hand the probability of rain for a day, given that the forecast said it is going to rain.\n","\n","I suggest you proceed as follows: (i) fill the data you know in an events probability [table](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables), as seen in the lecture; (ii) remember that probabilities sum to a constant over all possible events, so (make a copy of it and) fill in the blanks; (iii) state very clearly what are the posterior, prior, likelihood and evidence; (iv) only assemble your Bayes' equation once you are certain of your components."]},{"cell_type":"markdown","metadata":{"id":"CqklzsE54UhM","colab_type":"text"},"source":["Let's translate first our human readable data into math equations:\n","\n","- Let $A =$ \"Forecast says it's going to rain\" (hypothesis)\n","- Let $B =$ \"It's going to rain\" (observation)\n","\n","Now, \n","\"When the weather forecast says it is going to rain, 15% of the time they are wrong.\" means:\n","- $P(not B|A) = 0.15$ \n","\n","And, similarly: \"When they say it is not going to rain, they are wrong 5% of the times.\" translates into\n","- $P(B| not A) = 0.05$\n","\n","And, also \"Also, hypothesize that in the current season you get rain on 20% of the days.\"\n","- $P(B) = 0.2$\n","\n","\n","Which gives us:\n","\n","| -                     | B (rain)      | not B       | total  |\n","|:---------------------:|:-------------:|:-----------:|:------:|\n","| **A (forecast rain)** | x             | 0.15        | x+0.15 |\n","| **not A**             | 0.05          | y           | y+0.05 |\n","| **total**\t\t\t\t| 0.2\t  \t\t| 0.8    \t  | 1      |\n","\n","\n","Filling the gaps such that the rows and columns sum up the right way, gives us:\n","\n","| -                     | B (rain)      | not B       | total  |\n","|:---------------------:|:-------------:|:-----------:|:------:|\n","| **A (forecast rain)** | 0.15          | 0.15        | 0.3    |\n","| **not A**                | 0.05          | 0.65        | 0.7    |\n","| **total**\t\t\t| 0.2\t  \t\t| 0.8     \t  | 1      |\n","\n","\n","And finally, \n","\n","$$\n","P(B|A) = \\frac{P(B,A)}{P(B)} = \\frac{P(B)\\cdot P(A|B)}{P(A)} = \\frac{ 0.2\\cdot \\frac{0.15}{0.2}}{0.3} = 0.50\n","$$"]},{"cell_type":"markdown","metadata":{"id":"n7bPseoO4UhN","colab_type":"text"},"source":["#### 1.2 **[1pt]** Explain $\\hat{y} = \\text{arg}\\!\\max_{y \\in Y}\\big\\{P(y \\,|\\, x)\\big\\}$ ."]},{"cell_type":"markdown","metadata":{"id":"qmU2YSKr4UhN","colab_type":"text"},"source":["As usual, $\\hat y$ is the prediction, the predicted label. When we say $argmax_{x\\in Y}$, we take one particular $y \\in Y$, for which we have that the function value of $P(y|x)$ is maximal. Thus take that $y \\in Y$ for which the probability to happen given $x$ happened is the highest of all $y \\in Y$. \n","\n","This way, we take that label with the highest probability (to happen) and minimize the mistake we can make (statistically)."]},{"cell_type":"markdown","metadata":{"id":"aKH1bgg24UhO","colab_type":"text"},"source":["#### 1.3 **[1pt]** How does NB differ from LDA in maintaining the covariance of the distributions modeling the data?\n"]},{"cell_type":"markdown","metadata":{"id":"9R_WuDxG4UhP","colab_type":"text"},"source":["Using NB we lose the \"dependence\" of the features, information relating two features together. LDA maintains the covariance between the features. But as seen in the lecture, citation: *We lose a bit of the modeling power in exchange for a lot more flexibility and performance.*"]},{"cell_type":"markdown","metadata":{"id":"CJUd4BOi4UhQ","colab_type":"text"},"source":["# 2. Model Selection for Naïve Bayes"]},{"cell_type":"markdown","metadata":{"id":"_qsci-xJ4UhR","colab_type":"text"},"source":["#### 2.1 **[3pt]** Load the `tips` dataset from Seaborn  (into a Pandas DataFrame). Which distribution would you use to model each of the features in the dataset? Explain your choices.\n","\n","You load the dataset the same way you did for `iris` before. Obviously you need to study it to be able to answer. You should find useful to consider (i) the list of dtypes for each feature, (ii) the number of unique values for each of the categorical features, (iii) you can use the `pairplot` to quickly inspect the data: can you do better than a simple Gaussian if there are multiple peaks or asymmetry in the distribution of the real-valued features?  \n","The code cell below is to hold your analysis, while the real answer + motivations go in the Markdown cell just underneath."]},{"cell_type":"code","metadata":{"id":"_xtxqqio4UhS","colab_type":"code","outputId":"ef7cd66a-adca-42fb-83e1-33ca2cea987e","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1585224222222,"user_tz":-60,"elapsed":795,"user":{"displayName":"n/a","photoUrl":"","userId":"05363706501552199507"}}},"source":["pd.set_option('display.max_rows', None)\n","tips = sns.load_dataset(\"tips\")\n","sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")\n","\n","\n","\n","#dtypes to consider\n","print(tips.dtypes)\n","#unique values\n","print(tips.nunique(axis = 0))\n","\n","## To identify distributions, use this!\n","#cat_columns = tips.select_dtypes(['category']).columns\n","#tips[cat_columns] = tips[cat_columns].apply(lambda x: x.cat.codes)\n","#sns.pairplot(tips)\n","#\n","## To look at the real-valued fearures, distinct between them with hue\n","#sns.pairplot(tips, hue=\"time\")\n","#sns.pairplot(tips, hue=\"sex\")\n","#sns.pairplot(tips, hue=\"smoker\")\n","#sns.pairplot(tips, hue=\"day\")\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["total_bill     float64\n","tip            float64\n","sex           category\n","smoker        category\n","day           category\n","time          category\n","size             int64\n","dtype: object\n","total_bill    229\n","tip           123\n","sex             2\n","smoker          2\n","day             4\n","time            2\n","size            6\n","dtype: int64\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-g0JbgxX4UhV","colab_type":"text"},"source":["Use the first part to visually see the distributions for the discrete features:\n","\n","- total_bill, tip (real valued): simple gaussian/normal\n","- sex, smoker, time (binary): bernoulli\n","- day, size (discreet, more than two): binomial\n","\n","And then use the second part with the hue to see that we have mixture-gaussians, where we could model the distribution with two or more gaussian distributions for the real-valued features total_bill and tip."]},{"cell_type":"markdown","metadata":{"id":"0E9vrVPT4UhW","colab_type":"text"},"source":["# 3. Naïve Bayes Classification"]},{"cell_type":"markdown","metadata":{"id":"e_8YV1By4UhW","colab_type":"text"},"source":["Let's write a Naïve Bayes classifier from scratch. We will work with the `iris` dataset (again, from Seaborn) since we know already the data. All features are continuous: for simplicity we can use simple Gaussians, but we should expect some misclassification.\n","\n","From now on let's also introduce the train-test split so we can start verifying our model's performance the right way. Just use `train` for your answer instead of `df`, and leave `test` for the end."]},{"cell_type":"code","metadata":{"id":"gtUsIe494UhX","colab_type":"code","colab":{}},"source":["df = sns.load_dataset('iris')\n","\n","from sklearn.model_selection import train_test_split\n","train, test = train_test_split(df, test_size=0.2) # 80-20 split"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FycK_NKe4UhZ","colab_type":"text"},"source":["#### 3.1 **[2pt]** Compute the priors for the three classes of the Iris dataset using the Pandas DataFrame, _in a single line of code_ and without using loops (`for`, `while`, etc.).\n","\n","One-liners are typically bad practice, but here I need to force you to learn this new tool and stop writing `for` loops, since they will not scale from now on.  \n","\n","Careful as many tutorials online (such as [this one](https://chrisalbon.com/machine_learning/naive_bayes/naive_bayes_classifier_from_scratch/) will explicitly select the class and run the same calculation multiple times (and in multiple lines). This approach **does not scale** to problems with 100 or 10'000 classes: learn to use `groupby()` instead!  \n","_[Think: this course should make you confident enough to be the one writing the tutorials, and hopefully of much better quality!]_\n","\n","As a reference, you will need to (i) group the dataframe by species, (ii) select only the grouped elements (returning a Series), (iii) run the Numpy-backed `count()`, (iv) divide by the total number of elements.  "]},{"cell_type":"code","metadata":{"id":"rKGA8wbJ4UhZ","colab_type":"code","colab":{}},"source":["priors = train.groupby(['species']).size().div(train.shape[0])\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xJXfPcca4Uhb","colab_type":"text"},"source":["#### 3.2 **[1pt]** Compute the means and the standard deviations for each feature and for each class of the Iris dataset using the Pandas DataFrame (one line of code each).\n","\n","As a reference, you should obtain 12 means and 12 standard deviations. Again, the use of `groupby` followed by Numpy's functions will take literally 2 lines and no loops. Remember to use the `train` data!"]},{"cell_type":"code","metadata":{"id":"5kU5LuGo4Uhc","colab_type":"code","colab":{}},"source":["stds = train.groupby(['species']).std();\n","means = train.groupby(['species']).mean();"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8iFYruM4Uhe","colab_type":"text"},"source":["Here is a freebie to save you some debugging time: the (stunted) equation for the Gaussian probability. Stunted in the sense that, since it is only used to maximize the class probability, parts that do not depend on the class have been dropped (as usual). It requires you to define first the variables `means` and `stds` from the previous question (both $(3\\times4)$ DataFrames).\n","\n","If you really want to understand what is going on (especially with Pandas), I challenge you to comment it out, pull the slides, and write your own. You did something very close for LDA, feel free to review your code. You don't need it to look identical as long as it does the same job.\n","\n","Remember that Naïve Bayes computes the class likelihood as a product of the independent probabilities for each feature: this is done by the `product()` on the columns. If you remove that, you should have 12 values (give it a try).\n","\n","When passing a line of input to `likelihood` be careful to remove the last column (the `species`) as in the example below (in our previous calculations this was done by the `groupby()`, which made a new dimension out of it)."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"-uESdaLw4Uhf","colab_type":"code","outputId":"98f19c2e-0b3d-4aa8-d440-787154f59238","colab":{"base_uri":"https://localhost:8080/","height":102},"executionInfo":{"status":"ok","timestamp":1585224222429,"user_tz":-60,"elapsed":979,"user":{"displayName":"n/a","photoUrl":"","userId":"05363706501552199507"}}},"source":["likelihood = lambda x: (np.exp(-(x-means)**2/(2*stds**2))/stds).product(axis=1)\n","likelihood(train.iloc[50, :-1]).round(3)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["species\n","setosa         0.000\n","versicolor     0.001\n","virginica     20.650\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"K9rF9VYv4Uhh","colab_type":"text"},"source":["#### 3.3 **[1pt]** Write a Python function that predicts the class of input $x$ (i.e. returns $\\hat{y}$ for one line of data).\n","\n","As a sanity check: it should take a row as input (without labels, as for `likelihood` above) and return the string found in the `index` of the max value (the documentation is your friend)."]},{"cell_type":"code","metadata":{"id":"VStM-2Ak4Uhh","colab_type":"code","outputId":"780070df-a23b-463d-9e14-e87f4fbb4bb1","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1585224225978,"user_tz":-60,"elapsed":573,"user":{"displayName":"n/a","photoUrl":"","userId":"05363706501552199507"}}},"source":["predictor = lambda x: (priors*likelihood(x)).idxmax(axis = 0) \n","\n","predictor(train.iloc[50, :-1])"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'virginica'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"_uNqJ66Q4Uhj","colab_type":"text"},"source":["#### 3.4 **[2pt]** Compute $\\hat{y}$ for all points in the `test` dataset, in one line and without using Python loops (`for`, `while`, etc.). Compare it with the correct label $y$ and print the number of misclassified points.\n","\n","And here is how you use the test set: after the training on the train set is complete, you evaluate its performance on data it was not trained on. This is absolutely **crucial** in machine learning. We will use this process from now on, and using the wrong dataset (either for training or testing) will be considered a major error (so careful with typos! Double-check every time!). If you wonder why so strict, check again the 4th lecture and ask yourself what are the consequences of getting it wrong in a work or research setting (and feel free to discuss on Moodle).\n","\n","Again, no loops: you need both to drop the last column and then to apply the function to the rows. For example: `train.iloc[:, :-1].apply(my_predict_fn, axis = 1)`. Can you make it look nicer/more readable?\n","\n","Remember you can count the number of `True` values in a numpy array simply by calling `sum()` on it."]},{"cell_type":"code","metadata":{"id":"GKTdQRJM4Uhj","colab_type":"code","outputId":"0a404f7c-635d-493f-b55e-a462b8af937b","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1585224233378,"user_tz":-60,"elapsed":779,"user":{"displayName":"n/a","photoUrl":"","userId":"05363706501552199507"}}},"source":["predicted = test.iloc[:, :-1].copy().apply(predictor, axis = 1)\n","\n","matches = np.where(test['species'] == predicted, True, False)\n","\n","\"misclassified\",len(test)-sum(matches)"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('misclassified', 0)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"lgk78M-y4Uhk","colab_type":"text"},"source":["#### 3.5 **[1pt]** Why did we not compute (nor need) the _evidence_ for predicting the input's class?"]},{"cell_type":"markdown","metadata":{"id":"pWTAYWp-4Uhl","colab_type":"text"},"source":["Because it does not depend on $y$.\n","We try to maximize Naïve Bayes, thus we try to maximize:\n","$$\n","P(y|x) = \\frac{P(y)\\cdot P(x|y)}{P(x)}\n","$$\n","\n","The evidence $P(x)$ does not depend on $y$, therefore we can drop it. \n","\n","Basically, if you have a list:\n","```\n","{\n","  1: 5/9\n","  2: 4/9\n","  3: 8/9\n","}\n","```\n","\n","\n","The max of this list is the value with index 3. The same would be true if we would drop the /9 for every entry."]},{"cell_type":"markdown","metadata":{"id":"J5W5OJ0Y4Uhl","colab_type":"text"},"source":["#### 3.6 **[2pt]** Train a scikit-learn Naïve Bayes Gaussian classifier on the Iris train data using a Pandas Dataframe, and print the number of misclassified points on the test data.\n","\n","Remember that:\n","- Now that we have a bit more experience with Pandas we can learn how to pass the DataFrames directly to scikit-learn.\n","- The training data should always be 2D (i.e. DataFrame) and not have the label (`train.iloc[:,:-1]`, do you know what each `:` stands for?).\n","- The labels should always be 1D (i.e. Series) and numerical. Rather than doing the conversion manually, you should convert the feature to categorical and then use its codes (`train['species'].astype('category').cat.codes`).\n","- Mistakenly testing on the train set will fail the question, as will comparing the prediction against the train set labels (hint hint).\n","- You will probably get better results with scikit-learn because it uses multivariate Gaussians and improved estimators (check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html))."]},{"cell_type":"code","metadata":{"id":"1dLiyym84Uhm","colab_type":"code","outputId":"9f19835d-2dec-4209-8c0d-0743bfb5d958","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from sklearn.naive_bayes import GaussianNB\n","clf = GaussianNB()\n","clf.fit(train.iloc[:,:-1], train['species'])\n","GaussianNB()\n","\n","predicted = clf.predict(test.iloc[:,:-1])\n","\n","np.where(test['species'] == predicted, True, False)\n","\n","\"misclassified\",len(test)-sum(matches)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('misclassified', 0)"]},"metadata":{"tags":[]},"execution_count":155}]},{"cell_type":"markdown","metadata":{"id":"ddJORVLS4Uhn","colab_type":"text"},"source":["# At the end of the exercise"]},{"cell_type":"markdown","metadata":{"id":"I2OA0-ct4Uho","colab_type":"text"},"source":["Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n","\n","The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"]},{"cell_type":"markdown","metadata":{"id":"yku3cGDH4Uho","colab_type":"text"},"source":["#### BONUS **[ZERO pt]** Do a bit of independent research, and propose below the simplest example you can, to make evident how the frequentist and Bayesian approaches are different.\n","\n","I advise against blind copy+paste from the Internet in this case, I have seen so many incorrect opinions and tutorials over the years it is frankly ridiculous. I suggest you rather argue a bit on the Moodle about the approaches themselves, so you can make sure your example is correct.\n","\n","A good intro: [[link]](https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf)."]},{"cell_type":"markdown","metadata":{"id":"FwSrgIJ44Uhp","colab_type":"text"},"source":["#### BONUS **[ZERO pt]** Train a Gaussian NB (either your code or scikit-learn) on the full Iris dataset (no train-test split) and check the misclassifications. Train the same on the 80% training data, then check and aggregate misclassifications both on the train and test datasets. You will probably get the same number of total errors regardless of whether you trained on 80% or 100% of the data. Can you explain why? Feel free to play with different splits until you find how low can you go with the training before increasing the number of errors. Use the term `statistically representative` in your explanation."]},{"cell_type":"markdown","metadata":{"id":"pjg7WviI4Uhp","colab_type":"text"},"source":["### Final considerations"]},{"cell_type":"markdown","metadata":{"id":"PVVhgwYh4Uhq","colab_type":"text"},"source":["- This is the first core ML method we are covering in the course. As you see it expects you to know quite a lot of concepts before we can really discuss its workings.\n","- This is also the first method capable of nonlinear classification. As it can work with multiple classes and different types of distributions (think Mixture of Gaussians), the division boundary is not a line anymore.\n","- In the next two lectures we will ease into the other big gun of core ML, the Support Vector Machine with its infamous Kernel Trick. We are reaching the real complexity level of the course, and we will keep this difficulty until the exam. Follow closely both the lectures and exercises and you should have no trouble."]}]}