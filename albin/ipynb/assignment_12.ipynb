{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "assignment_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGBIT-atRqOC",
        "colab_type": "text"
      },
      "source": [
        "Please fill in your name and that of your teammate.\n",
        "\n",
        "You: Hans-Andrea Danuser\n",
        "\n",
        "Teammate:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc5XT07aRqOD",
        "colab_type": "text"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEhUxtaPRqOE",
        "colab_type": "text"
      },
      "source": [
        "Welcome to the twelfth lab. It's finally time to dedicate a whole lecture to the foundations of Reinforcement Learning, I hope the course so far has prepared you for this.\n",
        "\n",
        "There is a lot of coding, leveraging the fact that you covered DL last week; however a lot of code is already there, to limit the time you need. Nonetheless anything in the assignment could show up at the exam, so if there is anything unclear I suggest you ask on Moodle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw9ti8taRqOE",
        "colab_type": "text"
      },
      "source": [
        "### How to pass the lab?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6PmapJ0RqOF",
        "colab_type": "text"
      },
      "source": [
        "Below you find the exercise questions. Each question awarding points is numbered and states the number of points like this: **[0pt]**. To answer a question, fill the cell below with your answer (markdown for text, code for implementation). Incorrect or incomplete answers are in principle worth 0 points: to assign partial reward is only up to teacher discretion. Over-complete answers do not award extra points (though they are appreciated and will be kept under consideration). Save your work frequently! (`ctrl+s`)\n",
        "\n",
        "**You need at least 14 points (out of 21 available) to pass** (66%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w740lZwARqOF",
        "colab_type": "text"
      },
      "source": [
        "# 1. Fundamentals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1NArr8ZRqOG",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1 **[1pt]** Explain the Reinforcement Learning paradigm in English. Use the words Environment, Agent, Action and Feedback."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn8mlbBVRqOG",
        "colab_type": "text"
      },
      "source": [
        "In the RL paradigm, an agent and an enviroment are bound in an interaction-loop, where the agent send actionts to the enviroment and the enviroment will send a feedback. (typically a reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2UIzLVPRqOH",
        "colab_type": "text"
      },
      "source": [
        "#### 1.2 **[1pt]** Explain the equation for (pseudo-)Regret in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvOWiPEKRqOH",
        "colab_type": "text"
      },
      "source": [
        "The pseudoregret is the difference between the best possible reward obtained and the reward actually obtained. (always psoitive, goal have a regret as small as possible)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoBTtGjiRqOI",
        "colab_type": "text"
      },
      "source": [
        "#### 1.3 **[1pt]** Explain in English the importance of exploration and exploitation in MAB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC6Fo-1ARqOI",
        "colab_type": "text"
      },
      "source": [
        "The goal of exploration is to find the best arm (or action). Its done by choosing suboptimal action (or arms) in order to find better ones in the futur. The goal of exploitation is to use the best action(arm) in order to maximise the reward. this  is done by the agent choosing greedly. Their tradoff is the very heart of Reinforced Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pt8iDi7RqOJ",
        "colab_type": "text"
      },
      "source": [
        "#### 1.4 **[1pt]** Explain what would happen if you had a discount factor $\\gamma \\gt 1$ in the Ice Maze example from the lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahZmfig8RqOJ",
        "colab_type": "text"
      },
      "source": [
        "Futur rewards will have more value than immediate. Since there are no time restraints  the agent will learn not to reach the termination and try to \"wait\" aiming to maximise the reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUrbf_U4RqOK",
        "colab_type": "text"
      },
      "source": [
        "#### 1.5 **[2pt]** The Bellman Equation is at the heart of the classical Reinforcement Learning framework. Write it below in Latex, then explain each of the terms as if you were reading it in English."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oAMmXaGRqOK",
        "colab_type": "text"
      },
      "source": [
        "$\\nu_\\pi(s)= R(s,\\pi(s))+\\gamma \\sum_{s' \\in S} P^{\\pi(s)}_{s,s'} \\nu_\\pi(s')$\n",
        "\n",
        "The expectation of the future  reward of the agent by choosing policy $\\pi$, starting from the state $s$ is the sum of the reward obtained by executing the action chosen by $\\pi(s)$ in state $s$, plus the discounted value sum, for each state $s'$ of the probability of reaching $s'$ after executing $\\pi(s)$ in the state $s$, times the expected future reward collected by the agent following policy $\\pi$ starting from $s$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIIZCOJJRqOL",
        "colab_type": "text"
      },
      "source": [
        "# 2. Q-Learning from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSofLPPGY1rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import gym\n",
        "sns.set(rc={'figure.figsize':(8,6)}, style=\"whitegrid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBgBPCnQRqOL",
        "colab_type": "text"
      },
      "source": [
        "Time to get coding!\n",
        "- The [OpenAI Gym](https://gym.openai.com/) maintains a broad set of Reinforcement Learning benchmarks. It is ready to import on Colab, or you can add it to your local installation using `pipenv install gym`.\n",
        "- You can find the Ice Maze (named Frozen Lake) [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py).\n",
        "- The Classic Control games are also relatively easy to solve, and much nicer to render. Feel free to give it a try. You can save a video with `env = gym.wrappers.Monitor(env, 'video', force = True)` right after the `gym.make()` call. More on this topic [here](https://hub.packtpub.com/openai-gym-environments-wrappers-and-monitors-tutorial/).\n",
        "- To get you started, here is a working example of a random policy on the framework, which I took from their front page and customized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBhsK4aRRqOL",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1 **[2pt]** Write a Python function `choose_action` that takes as arguments `Q`, `state`, `epsilon`, `actions` and returns an action chosen according to Q-values using an epsilon-greedy policy.\n",
        "\n",
        "- `Q` is a dictionary that has states as keys. Each value is a numpy array with the Q-values corresponding to each action from the state used as key.\n",
        "- You cannot initialize `Q` with all possible states, because they can be infinite or unreachable. Using `Q[state]` with missing key `state` will throw an error; use instead `Q.get(state)` then check if its return `is None`.\n",
        "- `state` can be simply a number.\n",
        "- `actions` is a list of possible actions between which to choose.\n",
        "- Here is some code I used for testing:\n",
        "```python\n",
        "Q = {1:[0,0,1], 2:[0,1,0]}\n",
        "for _ in range(50):\n",
        "    print(choose_action(Q, 1, 0.0, []), end='') #=> 2\n",
        "print()\n",
        "for _ in range(50):\n",
        "    print(choose_action(Q, 2, 0.0, []), end='') #=> 1\n",
        "print()\n",
        "for _ in range(50):\n",
        "    print(choose_action(Q, 2, 1.0, range(3)), end='') #=> random action\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBOx0qQnRqOM",
        "colab_type": "code",
        "outputId": "ecfb6b5e-a878-478d-b3c9-27aa0b01c248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "def choose_action(Q, state, epsilon, actions):\n",
        "  q_vals = Q.get(state)\n",
        "  unexplored_state = q_vals is None\n",
        "  epsilon_chance = np.random.uniform(0.0,1.0) < epsilon\n",
        "\n",
        "  #so for the greedy part: if explore random choice:\n",
        "  if unexplored_state or epsilon_chance:\n",
        "    action = np.random.choice(actions)\n",
        "  #choose highest reward\n",
        "  else:\n",
        "    action = np.argmax(q_vals)\n",
        "\n",
        "  return action\n",
        "\n",
        "Q = {1:[0,0,1], 2:[0,1,0]}\n",
        "for _ in range(2):\n",
        "    print(choose_action(Q, 1, 0.0, []), end='') #=> 2\n",
        "print()\n",
        "for _ in range(50):\n",
        "    print(choose_action(Q, 2, 0.0, []), end='') #=> 1\n",
        "print()\n",
        "for _ in range(50):\n",
        "    print(choose_action(Q, 2, 1.0, range(3)), end='') #=> random action\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22\n",
            "11111111111111111111111111111111111111111111111111\n",
            "02112211201122122222022022111202221001021121211100"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtqKle4vRqOP",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2 **[2pt]** Write a Python function `update_Q` that takes as arguments `Q`, `state`, `action`, `next_state` and `num_actions`, and updates the `Q` dictionary according to Q-Learning. \n",
        "\n",
        "- Here you will need to initialize `Q[state]` if previously unexplored.\n",
        "- The future expected reward is the highest Q value from the next state -- or zero if unexplored. That's what the last argument is for.\n",
        "- I'll leave the testing to you. You may feel like just running the next question for testing (called _integration testing_ ), but testing it in isolation ( _unit testing_ ) is a much more controlled verification which should not be skipped (i.e. _easier_ )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Hvtmpi3RqOP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd94dbef-df92-4355-afe1-e2e2eaed10bd"
      },
      "source": [
        "def update_Q(Q, state, action, next_state, reward, num_actions, alpha, gamma):\n",
        "  if Q.get(state) is None:\n",
        "    Q[state] = np.zeros(num_actions)\n",
        "  \n",
        "  if Q.get(next_state) is None:\n",
        "    next_q = 0\n",
        "  else:\n",
        "    next_q = np.max(Q[next_state])\n",
        "\n",
        "  q_update = reward + gamma * next_q\n",
        "  Q[state][action] = (1-alpha) * Q[state][action] + alpha * q_update\n",
        "\n",
        "\n",
        "Q[3] = [0,0,0]\n",
        "update_Q(Q, state=3, action=2, next_state=2, reward=1, num_actions=3, alpha=0.6, gamma=0.3)\n",
        "\n",
        "print(Q[3])\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0.78]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOP-KbKmRqOR",
        "colab_type": "text"
      },
      "source": [
        "#### 2.3 **[1pt]** Run the code below to successfully use Q-Learning to solve the OpenAI Gym `FrozenLake` environment.\n",
        "\n",
        "- If you got both previous answers right, here is a free extra point for you :)\n",
        "- The parameters should work as they are, but feel free to play with them and make sure by the last epochs the environment is solved more often than not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dKUeATkRqOS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ea6d0eb5-6e4d-45e2-ce1b-4226bfe7262d"
      },
      "source": [
        "# Initialization and settings\n",
        "env = gym.make(\"FrozenLake-v0\", is_slippery=False)\n",
        "num_episodes = 1000\n",
        "max_nsteps = 30\n",
        "gamma = 0.9\n",
        "alpha = 0.6\n",
        "epsilon = 0.3\n",
        "\n",
        "num_actions = env.action_space.n\n",
        "Q = {} # hashing states to per-action reward arrays\n",
        "\n",
        "# Loop for each episode\n",
        "for ith_episode in range(num_episodes):\n",
        "\n",
        "    # Reset the environment (and obtain first observation)\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    # For each timestep\n",
        "    for t in range(max_nsteps):\n",
        "        \n",
        "        # Choose action according to Q-values using epsilon-greedy policy\n",
        "        # THIS SHOULD USE YOUR IMPLEMENTATION\n",
        "        action = choose_action(Q, state, epsilon, range(num_actions))\n",
        "        \n",
        "        # Execute action: get reward, move env to next state\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        # Add negative reward for falling in hole\n",
        "        if done and reward == 0: reward = -1\n",
        "\n",
        "        # Some useful printing to verify progression - (W)in or (l)ose\n",
        "        if reward == -1: print('l', end='', flush=True)\n",
        "        if reward == 1: print('W', end='', flush=True)\n",
        "\n",
        "        # Accumulate reward. Note this environment only rewards on termination though.\n",
        "        total_reward += reward\n",
        "        \n",
        "        # Update Q function for current state and action\n",
        "        # THIS SHOULD USE YOUR IMPLEMENTATION\n",
        "        update_Q(Q, state, action, next_state, reward, num_actions, alpha, gamma)\n",
        "        \n",
        "        # Update internal state\n",
        "        state = next_state\n",
        "        \n",
        "        # Terminate if episode ended\n",
        "        if done: break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "llllllllllllllllllllllllllllllllllllllllllllllllllllWlllllllllllllllWllllllllllllllllllllllllllllllllllllllllllllllllWllWWlWlWWWlWlWllWWlWlWWWWlWlWlWWllWWWWlWWWWlWWWWWlWllWlWWWWWWlWWWWlWWlWlWWWlWWWlWlWlWWWlllWllWWWlWllWllWWllWWWlWWWWWWWlllllWWWWWllllWWlWllllWWWlWWlWWWWlWWlWlWWWWlWWWlWWWWWWlWllllWlllWlWWWlWllWWlWWWllWWWWWlWlWlWlWWlWWlWWWWlWWWllWlWllWWWWWWWlWWWWWWWllWlWWlWWlWlWllWlWWlWWWlllWWWllllWllWWWllWllWWWWWWlWllllWlWWWWWWlWWWWllWWlllWlWlWlWWlWlWlWWlWlWllWWlWWWWlWllWWlWWWWWWWlWWllWWlWWWWWllWlWWWWWWWWWllllWlWWlWWlWWWWWllWWWWWWWWWWWlllllWWlWWWllWWWlWWWWWWWWWllWWlllWWWllWlWlWWWWWlllWWllllWWlWWWllWlWllllWWWWlWWllWlWlWlWWlWWWlWWWllWWWWlWWWWlWWWWWWlWlWllWWWlWllWlWWlWWWWlWWllWWWWWlWlWWWWlWWWWlWWWllWWWlWWWlWWWWlWWWWllllWlWWlWlWWlWWWllWllWWWWlWllWWWllWllWWWWWlWWWWWlWlWllWlWWWlWllWWWlWWlWWWWWWlWWWWWWlWWWWWWWlWlWWWlWWlWllWWWlWWWWWlWWWWWWWlWWWWWllWlWWlWWlWWllWWWWlllWllllWlWllWlWlWWWWWWlWlWWWWWWWlWWWWWWWlWllWlWlWWll"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfYPQxu4RqOV",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4 **[1pt]** Write a Python code snippet that satisfies the following requirements: (i) runs a fully-greedy policy, (ii) using the learned Q-values, (iii) rendering the environment in each step, and (iv) in less than 10 lines of code.\n",
        "\n",
        "- You can extract what you need from the code above -- if you know where to look and understand what you need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS5l9ecWRqOV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "91459005-e756-49f7-b3d2-68e63931c855"
      },
      "source": [
        "state = env.reset()\n",
        "\n",
        "for t in range(max_nsteps):\n",
        "  env.render()\n",
        "  action = np.argmax(Q[state])\n",
        "  state, reward, done, info = env.step(action)\n",
        "  if done:\n",
        "    env.render()\n",
        "    break"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "\u001b[41mF\u001b[0mFFH\n",
            "HFFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "F\u001b[41mF\u001b[0mFH\n",
            "HFFG\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "H\u001b[41mF\u001b[0mFG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "  (Right)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0qH9iIERqOY",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5 **[1pt]** Modify the code from question 2.3 to record the cumulative reward and number of time-steps for each episode. Then plot them with two line plots using Seaborn.\n",
        "\n",
        "- Think carefully: you need to augment the code above, by first initializing a proper data structure, then recording the statistics at the end of each episode. Finally plot them below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "DAT-qe0yRqOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GbPEDzZRqOa",
        "colab_type": "text"
      },
      "source": [
        "# 3. DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5osdFz0RqOb",
        "colab_type": "text"
      },
      "source": [
        "- DQN basically implements Q-Learning but uses a (deep) neural network to learn the rewards (with a few tricks). The main advantage versus the dictionary-based approach above is *generalization*: the network can output an expected reward for all actions, even if they were not explored yet.\n",
        "- If this code takes too long to execute, you may want to use [TPUs with Colab](https://www.tensorflow.org/guide/tpu)\n",
        "  - Though no idea if the `env.render()` will work, comment the line if it gives problems\n",
        "  - You can always save a video instead in your Google Drive\n",
        "- This code was originally taken from [this DQN tutorial](https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c) (with few edits). Study it to answer the following questions.\n",
        "- Learn more about the CartPole [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTvGk-6FRqOb",
        "colab_type": "code",
        "outputId": "15ef8a1b-7eaa-4860-cc53-ae9e73793164",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## IMPLEMENTATION ##\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# Do you know what a deque is? https://en.wikipedia.org/wiki/Deque\n",
        "# Though here it's used just to cap its number of elements\n",
        "from collections import deque\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, env):\n",
        "        self.env     = env\n",
        "        self.memory  = deque(maxlen=2000) # after 2000 will delete oldest record(s)\n",
        "\n",
        "        self.gamma = 0.85\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.01\n",
        "        self.tau = .125\n",
        "        self.batch_size = 64 # try different batch sizes\n",
        "        \n",
        "        self.model        = self.create_model()\n",
        "        self.target_model = self.create_model()\n",
        "\n",
        "    def create_model(self):\n",
        "        model = Sequential()\n",
        "        state_shape  = self.env.observation_space.shape\n",
        "        model.add(Dense(24, input_dim=state_shape[0], activation=\"relu\")) #bugfix\n",
        "        model.add(Dense(48, activation=\"relu\"))\n",
        "        model.add(Dense(24, activation=\"relu\"))\n",
        "        model.add(Dense(self.env.action_space.n))\n",
        "        model.compile(loss=\"mean_squared_error\",\n",
        "            optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def act(self, state):\n",
        "        self.epsilon *= self.epsilon_decay\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon)\n",
        "        if np.random.random() < self.epsilon: #bugfix\n",
        "            return self.env.action_space.sample()\n",
        "        return np.argmax(self.model.predict(state).flatten())\n",
        "\n",
        "    def remember(self, state, action, reward, new_state, done):\n",
        "        self.memory.append([state, action, reward, new_state, done])\n",
        "\n",
        "    def replay(self):\n",
        "        # This trick records past experience and re-plays for accelerate the training\n",
        "        if len(self.memory) < self.batch_size: return\n",
        "        samples = random.sample(self.memory, self.batch_size)\n",
        "        for sample in samples:\n",
        "            state, action, reward, new_state, done = sample\n",
        "            target = self.target_model.predict(state)\n",
        "            if done:\n",
        "                target[0][action] = reward\n",
        "            else:\n",
        "                Q_future = max(self.target_model.predict(new_state).flatten())\n",
        "                target[0][action] = reward + Q_future * self.gamma\n",
        "            self.model.fit(state, target, epochs=1, verbose=0)\n",
        "\n",
        "    # This part can be confusing: can you explain what is happening line by line?\n",
        "    def target_train(self):\n",
        "        weights = self.model.get_weights()\n",
        "        target_weights = self.target_model.get_weights()\n",
        "        for i in range(len(target_weights)):\n",
        "            target_weights[i] = weights[i] * self.tau + target_weights[i] * (1 - self.tau)\n",
        "        self.target_model.set_weights(target_weights)        "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egSWN0KeRqOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "ddc027b0-940b-498d-ed94-2f5609bec06c"
      },
      "source": [
        "## MAIN ##\n",
        "\n",
        "try: env.close() # your kernel can crash if you don't close the env properly\n",
        "except NameError: pass\n",
        "env     = gym.make(\"CartPole-v1\")\n",
        "gamma   = 0.9\n",
        "# Sometimes you will find code that uses (1-epsilon) for epsilon\n",
        "# Other times you will find a _decaying_ epsilon, lowering over time\n",
        "epsilon = .5 # it was .9 originally, play with it and understand its role\n",
        "\n",
        "ntrials   = 20 # I can see the beginning of learning with 20, feel free to raise this\n",
        "max_nstep = 200 # This environment is capped at 200 anyway, but you can try shorter\n",
        "\n",
        "dqn_agent = DQN(env=env)\n",
        "for trial in range(ntrials):\n",
        "    print(f\"Trial {trial+1}:\")\n",
        "    cur_state = env.reset().reshape(1,-1) # never forget to reset the env\n",
        "    tot_reward = 0\n",
        "    for step in range(max_nstep):\n",
        "        # print('.', end='', flush=True)\n",
        "        action = dqn_agent.act(cur_state)\n",
        "        print(action, end='', flush=True)\n",
        "        \n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        if done: reward = -100 # It's much easier to learn if you punish failing\n",
        "        tot_reward += reward\n",
        "\n",
        "        new_state = new_state.reshape(1,-1)\n",
        "        dqn_agent.remember(cur_state, action, reward, new_state, done)\n",
        "\n",
        "        dqn_agent.replay()       # Internally iterates default (prediction) model\n",
        "        dqn_agent.target_train() # What does this do?\n",
        "\n",
        "        cur_state = new_state #bugfix\n",
        "        if done: break\n",
        "    print(f\" Reward: {tot_reward}\")\n",
        "\n",
        "env.close()\n",
        "# Here is how to save trained Keras models\n",
        "# dqn_agent.model.save(\"trained.model\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1:\n",
            "011111001100111 Reward: -86.0\n",
            "Trial 2:\n",
            "1000011011011010110000011011100111 Reward: -67.0\n",
            "Trial 3:\n",
            "0101011111110010110 Reward: -82.0\n",
            "Trial 4:\n",
            "0001110000100000 Reward: -85.0\n",
            "Trial 5:\n",
            "01010110011010000101010 Reward: -78.0\n",
            "Trial 6:\n",
            "0010011101010100110101110001 Reward: -73.0\n",
            "Trial 7:\n",
            "00100100111101001011011001 Reward: -75.0\n",
            "Trial 8:\n",
            "001010100101100110001 Reward: -80.0\n",
            "Trial 9:\n",
            "000110001110011 Reward: -86.0\n",
            "Trial 10:\n",
            "011110101000101000011000011110011110010100011001101000110010 Reward: -41.0\n",
            "Trial 11:\n",
            "111001100110000 Reward: -86.0\n",
            "Trial 12:\n",
            "01100110111110011001000 Reward: -78.0\n",
            "Trial 13:\n",
            "10111111000000 Reward: -87.0\n",
            "Trial 14:\n",
            "11111100100 Reward: -90.0\n",
            "Trial 15:\n",
            "110111111011 Reward: -89.0\n",
            "Trial 16:\n",
            "01101111010100 Reward: -87.0\n",
            "Trial 17:\n",
            "101010100111100000101010110001110000111010100100000101100110100001001011101101111001 Reward: -17.0\n",
            "Trial 18:\n",
            "0110000010111111 Reward: -85.0\n",
            "Trial 19:\n",
            "0000001011111 Reward: -88.0\n",
            "Trial 20:\n",
            "1111100000000010110001111010100011100000010100001111101011 Reward: -43.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdrSJ0PGRqOg",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 **[3pt]** There are three bugs in the implementation. Fix them until it runs successfully.\n",
        "\n",
        "- One is in `## IMPLEMENTATION ##`, easy to find because it stops it from running.\n",
        "- One is just harder, messes with the policy. It's also in `## IMPLEMENTATION ##` but there's a hint in `## MAIN ##`.\n",
        "- One is a missing line in `## MAIN ##`: something there makes no sense.\n",
        "- Of course you can use the original implementation. But you will learn less and cannot do it at the exam, so give it a fair try first, because it's a fair exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rC68nQdRqOh",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 **[1pt]** Compare the shape of the model used with that of an autoencoder. Give one reason as to why they do or do not look alike."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV1mUx75RqOh",
        "colab_type": "text"
      },
      "source": [
        "autoencode: hourglass. this model: diamond shape. The autoencoder does that to support compact encoding, while this shape provides more second level features to support the decision-making levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rxxni-4RqOh",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 **[1pt]** How many inputs and outputs neurons does the model have? Write the code that answers the question. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJpSE7meRqOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a96cf8ca-4c93-46a2-804e-c641c82ec53b"
      },
      "source": [
        "print(env.observation_space.shape)\n",
        "print(env.action_space.n)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4,)\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZfphA-2RqOk",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4 **[3pt]** Write Python code to (i) create a new DQN agent, (ii) load a trained model into the agent's `model`, (iii) change the agent's epsilon-related parameters to enforce a greedy policy, and (iv) runs the greedy policy rendering each frame.\n",
        "\n",
        "- Remember to instantiate the environment first, and to close it at the end, or you may have problems.\n",
        "- Loading the model is easy -- if you saved it to a file in the `main` cell.\n",
        "- To enforce the greedy policy you need to set three agent variables, to sensible values.\n",
        "- You saw a similar evaluation loop when implementing Q-Learning. Just double check the differences with the DQN implementation to make sure it runs. You need to use `env.render()` here (if you're local; if you work on Colab just write a comment about it or save a video).\n",
        "- You can use `time.sleep()` to slow down the loop if the rendering is too fast. Remember also that the env's rendering window will close when you close the environment, so a pause there can also help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-IEnfYhRqOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import sleep\n",
        "import gym\n",
        "from keras.models import load_model\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "dqn_agent = DQN(env=env)\n",
        "dqn_agent.model = load_model(\"trained.model\")\n",
        "\n",
        "#greedy\n",
        "dqn_agent.epsilon = 0.0\n",
        "dqn_agent.epsilon_min = 0.0\n",
        "dqn_agent.epsilon_decay = 1.0\n",
        "\n",
        "max_nstep = 500\n",
        "state = env.reset().reshape(1,-1)\n",
        "\n",
        "for step in range(max_nstep):\n",
        "  env.render()\n",
        "  sleep(0.1)\n",
        "  action = dqn_agent.act(state)\n",
        "  print(action, end='', flush=True)\n",
        "  state, reward, done, _ = env.step(action)\n",
        "  state = state.reshape(1,-1)\n",
        "  if done: break\n",
        "\n",
        "sleep(1)\n",
        "env.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB4UFY5FRqOn",
        "colab_type": "text"
      },
      "source": [
        "# At the end of the exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd1zybYxRqOn",
        "colab_type": "text"
      },
      "source": [
        "Bonus question with no points! Answering this will have no influence on your scoring, not at the assignment and not towards the exam score -- really feel free to ignore it with no consequence. But solving it will reward you with skills that will make the next lectures easier, give you real applications, and will be good practice towards the exam.\n",
        "\n",
        "The solution for this questions will not be included in the regular lab solutions pdf, but you are welcome to open a discussion on the Moodle: we will support your addressing it, and you may meet other students that choose to solve this, and find a teammate for the next assignment that is willing to do things for fun and not only for score :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Pr_BSHtRqOo",
        "colab_type": "text"
      },
      "source": [
        "#### BONUS **[ZERO pt]** Solve another of the [OpenAI Gym environments](https://gym.openai.com/envs/#classic_control) by copying and modifying the above DQN implementation. This is very useful to learn to use different environments, and to see first-hand the limits of DRL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPjKZLXKRqOo",
        "colab_type": "text"
      },
      "source": [
        "#### BONUS **[ZERO pt]** Augment the DQN implementation to track episode size and cumulative reward and plot them with lineplots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qyANttjRqOp",
        "colab_type": "text"
      },
      "source": [
        "### Final considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKJn0keoRqOp",
        "colab_type": "text"
      },
      "source": [
        "- Reinforcement Learning is both the name of the learning paradigm and of the framework classically used to address such problems.\n",
        "- The amount of research currently dedicated to solve RL problems by improving both DL and the RL framework is staggering. Results still keep coming, but at a much slower pace than in purer SL applications (in front of orders of magnitude more investment).\n",
        "- Next week we will explore the limitations of the RL framework, and beyond."
      ]
    }
  ]
}